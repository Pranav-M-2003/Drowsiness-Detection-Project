{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b1be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf ## pip install tensorflow-gpu\n",
    "import cv2 ### pip install opencv-python\n",
    " ## pip install opencv-contrib-python  fullpackage\n",
    "import matplotlib.pyplot as plt ## pip install matplotlib\n",
    "import numpy as np ## pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd29990",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datadirectory = \"dataset\" ## training dataset\n",
    "Classes = [\"Closed_eyes\",\"Open_eyes\"] ## list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3134a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_Data = []\n",
    "\n",
    "def create_training_Data():\n",
    "  for category in Classes:\n",
    "    path = os.path.join(Datadirectory, category)\n",
    "    class_num = Classes.index(category) ## 0 1,\n",
    "    for img in os.listdir(path):\n",
    "      try:\n",
    "          img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "          backtorgb = cv2.cvtColor(img_array,cv2.COLOR_GRAY2RGB)\n",
    "          new_array = cv2.resize(backtorgb, (img_size,img_size))\n",
    "          training_Data.append([new_array,class_num])\n",
    "      except Exception as e:\n",
    "          pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9c48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_Data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66cf1039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d27e9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features.label in training_Data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "X = X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df111273",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5956ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dd08646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06a587f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f905c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
      "17225924/17225924 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18aa812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input = model.layers[0].input ## input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f27db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output = model.layers[-4].output ## output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da0342af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flat_layer = layers.Flatten()(base_output)\n",
    "final_output = layers.Dense(1)(Flat_layer) ## one node (1/0)\n",
    "final_output = layers.Activation('sigmoid')(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84ce4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.Model(inputs = base_input, outputs= final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c8f947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss=\"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b287b8a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.1`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_model\u001b[38;5;241m.\u001b[39mfit(X,Y, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1785\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1782\u001b[0m split_at \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(batch_dim \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m validation_split)))\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m batch_dim:\n\u001b[1;32m-> 1785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data contains \u001b[39m\u001b[38;5;132;01m{batch_dim}\u001b[39;00m\u001b[38;5;124m samples, which is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1787\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msufficient to split it into a validation and training set as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1788\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified by `validation_split=\u001b[39m\u001b[38;5;132;01m{validation_split}\u001b[39;00m\u001b[38;5;124m`. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide more data, or a different value for the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_split` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1791\u001b[0m             batch_dim\u001b[38;5;241m=\u001b[39mbatch_dim, validation_split\u001b[38;5;241m=\u001b[39mvalidation_split\n\u001b[0;32m   1792\u001b[0m         )\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split\u001b[39m(t, start, end):\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.1`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "new_model.fit(X,Y, epochs = 1,validation_split = 0.1) ## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ec1b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9906dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9552dba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57fe3067",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m eyes \u001b[38;5;241m=\u001b[39m eye_cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray,\u001b[38;5;241m1.1\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gray' is not defined"
     ]
    }
   ],
   "source": [
    "eyes = eye_cascade.detectMultiScale(gray,1.1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f50a5006",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eyes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m(x,y,w,h) \u001b[38;5;129;01min\u001b[39;00m eyes:\n\u001b[0;32m      2\u001b[0m   cv2\u001b[38;5;241m.\u001b[39mrectangle(img,(x,y),(x\u001b[38;5;241m+\u001b[39mw,y\u001b[38;5;241m+\u001b[39mh),(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eyes' is not defined"
     ]
    }
   ],
   "source": [
    "for(x,y,w,h) in eyes:\n",
    "  cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef27f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Realtime Video Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cb2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "## Checking if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "  cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "  raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #print(faceCascade.empty())\n",
    "    eyes = eye_cascade.detectMultiScale(gray,1.1,4)\n",
    "    for x,y,w,h in eyes:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h),(0,255,0),2)\n",
    "        eyess =  eye.cascade.detectMultiScale(roi_gray)\n",
    "        if len(eyess) == 0:\n",
    "            print(\"eyes are not detected\")\n",
    "        else:\n",
    "            for (ex,ey,ew,eh) in eyess:\n",
    "                eyes_roi = roi_color[ey: ey+eh, ex:ex + ew]\n",
    "\n",
    "        final_image = cv2.resize(eyes_roi,(224,224))\n",
    "        final_image = np.expand_dims(final_image,axis =0)\n",
    "        final_image = final_image/255.0\n",
    "\n",
    "        Predictions = new_model.predict(final_image)\n",
    "        if(Predictions>0):\n",
    "            status = \"Open Eyes\"\n",
    "        else:\n",
    "            status = \"Closed Eyes\"\n",
    "\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        print(faceCascade.empty())\n",
    "        faces = faceCascade.detectMultiScale(gray,1.1,4)\n",
    "\n",
    "        ## Draw a rectangle around the faces\n",
    "        for(x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "        ## Using putText() method for inserting text on video\n",
    "\n",
    "        cv2.putText(frame,status,(50,50),font,3,(0,0,255),2,cv2.LINE_4)\n",
    "        cv2.imshow('Drowsiness Detection',frame)\n",
    "\n",
    "        if cv2.waitKey(2) &  0xFF == odd('q'):\n",
    "          break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
